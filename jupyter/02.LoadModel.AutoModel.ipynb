{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b64626-3618-470d-b8e0-279b2e9e3975",
   "metadata": {},
   "source": [
    "# glm4 streamè°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce2761b-1578-4e8d-ad4c-5dcd7a4c3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deeb64de51646679d4cca55bcd25076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer, AutoModel\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "MODEL_PATH = '/opt/Data/ModelWeight/THUDM/glm-4-9b-chat'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    encode_special_tokens=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917c5745-7ef7-4506-896d-fe25b0494fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM-4:\n",
      "å­©å­æ¯å¤©å–œæ¬¢çŽ©è€è€Œä¸å¤ªçƒ­è¡·äºŽå­¦ä¹ çš„çŽ°è±¡åœ¨å„¿ç«¥å¿ƒç†å­¦ä¸­æ˜¯æ­£å¸¸ä¸”æ™®éçš„ã€‚ä»¥ä¸‹æ˜¯å¯¹è¿™ä¸€çŽ°è±¡çš„ä¸€äº›çœ‹æ³•ï¼š\n",
      "\n",
      "1. **ç”Ÿç†å’Œå¿ƒç†å‘å±•ç‰¹ç‚¹**ï¼š\n",
      "   - å„¿ç«¥æ—¶æœŸæ˜¯äººç±»å¤§è„‘å‘è‚²æœ€å¿«çš„é˜¶æ®µä¹‹ä¸€ï¼Œå­©å­ä»¬åœ¨è¿™ä¸ªæ—¶æœŸçš„æ³¨æ„åŠ›ã€è‡ªæŽ§åŠ›å’Œè®¤çŸ¥èƒ½åŠ›è¿˜åœ¨å‘å±•ä¸­ã€‚\n",
      "   - çŽ©è€å¯¹äºŽå­©å­çš„èº«å¿ƒå¥åº·å‘å±•è‡³å…³é‡è¦ï¼Œå®ƒæœ‰åŠ©äºŽä»–ä»¬æŽ¢ç´¢ä¸–ç•Œã€åŸ¹å…»ç¤¾äº¤æŠ€èƒ½å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚\n",
      "\n",
      "2. **æ•™è‚²ç†å¿µ**ï¼š\n",
      "   - å½“å‰çš„æ•™è‚²è§‚å¿µè¶Šæ¥è¶Šå¼ºè°ƒâ€œå¿«ä¹å­¦ä¹ â€ï¼Œå³é€šè¿‡æ¸¸æˆåŒ–çš„æ–¹å¼è®©å­©å­åœ¨å­¦ä¹ ä¸­èŽ·å¾—ä¹è¶£ï¼Œä»Žè€Œæé«˜ä»–ä»¬çš„å­¦ä¹ å…´è¶£å’Œå­¦ä¹ æ•ˆæžœã€‚\n",
      "   - å¼ºè°ƒç´ è´¨æ•™è‚²è€Œéžå•çº¯çš„åº”è¯•æ•™è‚²ï¼Œé¼“åŠ±å­©å­åœ¨å¾·æ™ºä½“ç¾ŽåŠ³å„æ–¹é¢å…¨é¢å‘å±•ã€‚\n",
      "\n",
      "3. **å®¶åº­æ•™è‚²ä¸Žå¼•å¯¼**ï¼š\n",
      "   - å®¶é•¿å¯ä»¥é€šè¿‡åˆ›é€ è‰¯å¥½çš„å®¶åº­çŽ¯å¢ƒæ¥æ¿€å‘å­©å­çš„å­¦ä¹ å…´è¶£ï¼Œæ¯”å¦‚æä¾›ä¸°å¯Œçš„å›¾ä¹¦èµ„æºã€ç§‘å­¦å®žéªŒå™¨æç­‰ã€‚\n",
      "   - é€šè¿‡äº²å­æ´»åŠ¨æˆ–å‚ä¸Žç¤¾ä¼šå®žè·µç­‰æ–¹å¼ï¼Œå°†å­¦ä¹ å†…å®¹èžå…¥æ—¥å¸¸ç”Ÿæ´»ï¼Œä½¿å­©å­åœ¨å®žé™…æ“ä½œä¸­å‘çŽ°çŸ¥è¯†çš„é‡è¦æ€§ã€‚\n",
      "\n",
      "4. **ç¤¾ä¼šå› ç´ **ï¼š\n",
      "   - åœ¨ç«žäº‰æ¿€çƒˆçš„ç¤¾ä¼šçŽ¯å¢ƒä¸­ï¼Œå®¶é•¿å¯èƒ½ä¼šå¯¹å­©å­æœ‰è¾ƒé«˜çš„æœŸæœ›å€¼ï¼Œè¿™å¯èƒ½å¯¼è‡´å­©å­æ„Ÿåˆ°åŽ‹åŠ›é‡é‡ï¼Œä¸æ„¿æ„ä¸»åŠ¨åŽ»å­¦ä¹ ã€‚\n",
      "   - å­¦æ ¡çš„æ•™è‚²æ–¹å¼å’Œè¯„ä»·ä½“ç³»ä¹Ÿå¯èƒ½å½±å“å­¦ç”Ÿçš„å­¦ä¹ ç§¯æžæ€§ã€‚\n",
      "\n",
      "5. **å¹³è¡¡çŽ©è€ä¸Žå­¦ä¹ **ï¼š\n",
      "   - ä½œä¸ºå®¶é•¿å’Œæ•™è‚²è€…ï¼Œåº”å½“æ‰¾åˆ°çŽ©è€ä¸Žå­¦ä¹ ä¹‹é—´çš„å¹³è¡¡ç‚¹ï¼Œæ—¢ä¸å‰¥å¤ºå­©å­åº”æœ‰çš„ç«¥å¹´ä¹è¶£ï¼Œä¹Ÿä¸å¿½è§†å…¶å­¦ä¸šæˆé•¿ã€‚\n",
      "   - é¼“åŠ±å­©å­è¿›è¡Œæœ‰ç›Šèº«å¿ƒçš„æˆ·å¤–æ´»åŠ¨å’Œé›†ä½“è¿åŠ¨ï¼ŒåŒæ—¶åˆç†å®‰æŽ’è¯¾ä¸šè´Ÿæ‹…ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œå¯¹å¾…å­©å­å–œæ¬¢çŽ©è€è€Œè¾ƒå°‘çƒ­çˆ±å­¦ä¹ çš„æƒ…å†µï¼Œåº”è¯¥é‡‡å–ç†è§£å’Œæ”¯æŒçš„æ€åº¦ï¼Œç»“åˆç§‘å­¦çš„è‚²å„¿æ–¹æ³•ï¼Œå¸®åŠ©å­©å­å¥åº·æˆé•¿ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå®¶é•¿çš„è€å¿ƒå’Œç†è§£å°¤ä¸ºé‡è¦ã€‚"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "max_length = 8192\n",
    "top_p = 0.8\n",
    "temperature = 0.6\n",
    "stop = StopOnTokens()\n",
    "\n",
    "user_input = \"å¦‚ä½•çœ‹å¾…å°å­©å­æ¯å¤©éƒ½å–œæ¬¢çŽ©ï¼Œä¸å¤ªçˆ±å­¦ä¹ \"\n",
    "history.append([user_input, \"\"])\n",
    "messages = []\n",
    "\n",
    "for idx, (user_msg, model_msg) in enumerate(history):\n",
    "    if idx == len(history) - 1 and not model_msg:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        break\n",
    "    if user_msg:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    if model_msg:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    timeout=60,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"input_ids\": model_inputs,\n",
    "    \"streamer\": streamer,\n",
    "    \"max_new_tokens\": max_length,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": top_p,\n",
    "    \"temperature\": temperature,\n",
    "    \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"eos_token_id\": model.config.eos_token_id,\n",
    "}\n",
    "\n",
    "t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "t.start()\n",
    "\n",
    "print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "for new_token in streamer:\n",
    "    if new_token:\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "        history[-1][1] += new_token\n",
    "\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bf099a-f593-4224-93d9-2396d09b34fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM-4:\n",
      "ä½œä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæå‡å·¥ä½œæ•ˆçŽ‡æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®å¸®åŠ©æ‚¨æé«˜ä¸ªäººå·¥ä½œæ•ˆçŽ‡ï¼š\n",
      "\n",
      "1. **æ—¶é—´ç®¡ç†**ï¼š\n",
      "   - ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•ç­‰æŠ€å·§æ¥é›†ä¸­æ³¨æ„åŠ›ã€‚\n",
      "   - åˆ¶å®šæ¸…æ™°çš„æ—¥ç¨‹è¡¨å’Œä»»åŠ¡æ¸…å•ã€‚\n",
      "\n",
      "2. **ä¼˜å…ˆçº§æŽ’åº**ï¼š\n",
      "   - å­¦ä¼šåŒºåˆ†ç´§æ€¥ä¸Žé‡è¦çš„äº‹åŠ¡ï¼Œå¹¶æŒ‰ç…§é‡è¦æ€§è¿›è¡ŒæŽ’åºå¤„ç†ã€‚\n",
      "\n",
      "3. **æŠ€æœ¯å·¥å…·**ï¼š\n",
      "   - åˆ©ç”¨ç‰ˆæœ¬æŽ§åˆ¶ã€é¡¹ç›®ç®¡ç†å·¥å…·ï¼ˆå¦‚Jiraï¼‰ã€ä»£ç å®¡æŸ¥å·¥å…·ç­‰è‡ªåŠ¨åŒ–æµç¨‹ã€‚\n",
      "   - å­¦ä¹ ä½¿ç”¨å¿«æ·é”®å’Œè„šæœ¬æ¥è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡ã€‚\n",
      "\n",
      "4. **ç¼–ç ä¹ æƒ¯**ï¼š\n",
      "   - ä¿æŒè‰¯å¥½çš„ç¼–ç¨‹è§„èŒƒå’Œæ³¨é‡Šï¼Œä½¿ä»£ç æ˜“äºŽé˜…è¯»å’Œç»´æŠ¤ã€‚\n",
      "   - éµå¾ªè®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®žè·µã€‚\n",
      "\n",
      "5. **å›¢é˜Ÿåä½œ**ï¼š\n",
      "   - ä¸Žå›¢é˜Ÿæˆå‘˜ä¿æŒæ²Ÿé€šç•…é€šï¼ŒåŠæ—¶åˆ†äº«è¿›åº¦å’Œé—®é¢˜ã€‚\n",
      "   - å‚åŠ ä»£ç è¯„å®¡ä¼šè®®ï¼Œä»Žä»–äººåé¦ˆä¸­å­¦ä¹ ã€‚\n",
      "\n",
      "6. **çŸ¥è¯†ç§¯ç´¯**ï¼š\n",
      "   - æŒç»­å­¦ä¹ å’Œæ›´æ–°è‡ªå·±çš„æŠ€èƒ½åº“ã€‚\n",
      "   - å…³æ³¨è¡Œä¸šåŠ¨æ€å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚\n",
      "\n",
      "7. **é¿å…å¹²æ‰°**ï¼š\n",
      "   - åœ¨éœ€è¦ä¸“æ³¨å·¥ä½œæ—¶å…³é—­ä¸å¿…è¦çš„é€šçŸ¥å’Œç¤¾äº¤åª’ä½“åº”ç”¨ã€‚\n",
      "   - åˆ›å»ºä¸€ä¸ªæœ‰åˆ©äºŽå·¥ä½œçš„çŽ¯å¢ƒã€‚\n",
      "\n",
      "8. **å¥åº·ç”Ÿæ´»**ï¼š\n",
      "   - ä¿è¯å……è¶³çš„ç¡çœ å’Œé€‚å½“çš„è¿åŠ¨ã€‚\n",
      "   - æ³¨æ„é¥®é£Ÿå‡è¡¡ï¼Œä»¥ç»´æŒèº«ä½“å’Œç²¾ç¥žçŠ¶æ€è‰¯å¥½ã€‚\n",
      "\n",
      "9. **åæ€æ€»ç»“**ï¼š\n",
      "   - å®šæœŸå›žé¡¾è¿‡åŽ»çš„å·¥ä½œè¡¨çŽ°ï¼Œæ‰¾å‡ºå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚\n",
      "   - ä»Žå¤±è´¥çš„é¡¹ç›®æˆ–é”™è¯¯ä¸­å­¦åˆ°ç»éªŒæ•™è®­ã€‚\n",
      "\n",
      "10. **åˆç†åˆ†å·¥**ï¼š\n",
      "    - äº†è§£è‡ªå·±æ“…é•¿å’Œä¸æ“…é•¿çš„é¢†åŸŸï¼Œå°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„äººé€‰ã€‚\n",
      "\n",
      "11. **å‡å°‘ç­‰å¾…æ—¶é—´**ï¼š\n",
      "    - ä¼˜åŒ–å¼€å‘è¿‡ç¨‹ï¼Œç¼©çŸ­ç¼–è¯‘ã€æµ‹è¯•å’Œéƒ¨ç½²çš„æ—¶é—´ã€‚\n",
      "\n",
      "12. **å¿ƒç†è°ƒé€‚**ï¼š\n",
      "    - å­¦ä¼šæ”¾æ¾å¿ƒæƒ…ï¼Œé€‚æ—¶è°ƒæ•´å¿ƒæ€ã€‚\n",
      "    - å½“é‡åˆ°åŽ‹åŠ›æ—¶ï¼Œå¯»æ±‚åŒäº‹æˆ–ä¸“ä¸šäººå£«çš„å¸®åŠ©ã€‚\n",
      "\n",
      "é€šè¿‡ä»¥ä¸Šæ–¹æ³•ï¼Œæ‚¨å¯ä»¥é€æ­¥æé«˜å·¥ä½œæ•ˆçŽ‡ï¼Œä»Žè€Œæ›´å¥½åœ°å®Œæˆå·¥ä½œä»»åŠ¡ã€‚è®°ä½ï¼Œæ¯ä¸ªäººçš„æƒ…å†µä¸åŒï¼Œæ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ–¹æ³•æ˜¯å…³é”®ã€‚\n",
      "ä½œä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæå‡å·¥ä½œæ•ˆçŽ‡æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®å¸®åŠ©æ‚¨æé«˜ä¸ªäººå·¥ä½œæ•ˆçŽ‡ï¼š\n",
      "\n",
      "1. **æ—¶é—´ç®¡ç†**ï¼š\n",
      "   - ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•ç­‰æŠ€å·§æ¥é›†ä¸­æ³¨æ„åŠ›ã€‚\n",
      "   - åˆ¶å®šæ¸…æ™°çš„æ—¥ç¨‹è¡¨å’Œä»»åŠ¡æ¸…å•ã€‚\n",
      "\n",
      "2. **ä¼˜å…ˆçº§æŽ’åº**ï¼š\n",
      "   - å­¦ä¼šåŒºåˆ†ç´§æ€¥ä¸Žé‡è¦çš„äº‹åŠ¡ï¼Œå¹¶æŒ‰ç…§é‡è¦æ€§è¿›è¡ŒæŽ’åºå¤„ç†ã€‚\n",
      "\n",
      "3. **æŠ€æœ¯å·¥å…·**ï¼š\n",
      "   - åˆ©ç”¨ç‰ˆæœ¬æŽ§åˆ¶ã€é¡¹ç›®ç®¡ç†å·¥å…·ï¼ˆå¦‚Jiraï¼‰ã€ä»£ç å®¡æŸ¥å·¥å…·ç­‰è‡ªåŠ¨åŒ–æµç¨‹ã€‚\n",
      "   - å­¦ä¹ ä½¿ç”¨å¿«æ·é”®å’Œè„šæœ¬æ¥è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡ã€‚\n",
      "\n",
      "4. **ç¼–ç ä¹ æƒ¯**ï¼š\n",
      "   - ä¿æŒè‰¯å¥½çš„ç¼–ç¨‹è§„èŒƒå’Œæ³¨é‡Šï¼Œä½¿ä»£ç æ˜“äºŽé˜…è¯»å’Œç»´æŠ¤ã€‚\n",
      "   - éµå¾ªè®¾è®¡æ¨¡å¼å’Œæœ€ä½³å®žè·µã€‚\n",
      "\n",
      "5. **å›¢é˜Ÿåä½œ**ï¼š\n",
      "   - ä¸Žå›¢é˜Ÿæˆå‘˜ä¿æŒæ²Ÿé€šç•…é€šï¼ŒåŠæ—¶åˆ†äº«è¿›åº¦å’Œé—®é¢˜ã€‚\n",
      "   - å‚åŠ ä»£ç è¯„å®¡ä¼šè®®ï¼Œä»Žä»–äººåé¦ˆä¸­å­¦ä¹ ã€‚\n",
      "\n",
      "6. **çŸ¥è¯†ç§¯ç´¯**ï¼š\n",
      "   - æŒç»­å­¦ä¹ å’Œæ›´æ–°è‡ªå·±çš„æŠ€èƒ½åº“ã€‚\n",
      "   - å…³æ³¨è¡Œä¸šåŠ¨æ€å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚\n",
      "\n",
      "7. **é¿å…å¹²æ‰°**ï¼š\n",
      "   - åœ¨éœ€è¦ä¸“æ³¨å·¥ä½œæ—¶å…³é—­ä¸å¿…è¦çš„é€šçŸ¥å’Œç¤¾äº¤åª’ä½“åº”ç”¨ã€‚\n",
      "   - åˆ›å»ºä¸€ä¸ªæœ‰åˆ©äºŽå·¥ä½œçš„çŽ¯å¢ƒã€‚\n",
      "\n",
      "8. **å¥åº·ç”Ÿæ´»**ï¼š\n",
      "   - ä¿è¯å……è¶³çš„ç¡çœ å’Œé€‚å½“çš„è¿åŠ¨ã€‚\n",
      "   - æ³¨æ„é¥®é£Ÿå‡è¡¡ï¼Œä»¥ç»´æŒèº«ä½“å’Œç²¾ç¥žçŠ¶æ€è‰¯å¥½ã€‚\n",
      "\n",
      "9. **åæ€æ€»ç»“**ï¼š\n",
      "   - å®šæœŸå›žé¡¾è¿‡åŽ»çš„å·¥ä½œè¡¨çŽ°ï¼Œæ‰¾å‡ºå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚\n",
      "   - ä»Žå¤±è´¥çš„é¡¹ç›®æˆ–é”™è¯¯ä¸­å­¦åˆ°ç»éªŒæ•™è®­ã€‚\n",
      "\n",
      "10. **åˆç†åˆ†å·¥**ï¼š\n",
      "    - äº†è§£è‡ªå·±æ“…é•¿å’Œä¸æ“…é•¿çš„é¢†åŸŸï¼Œå°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„äººé€‰ã€‚\n",
      "\n",
      "11. **å‡å°‘ç­‰å¾…æ—¶é—´**ï¼š\n",
      "    - ä¼˜åŒ–å¼€å‘è¿‡ç¨‹ï¼Œç¼©çŸ­ç¼–è¯‘ã€æµ‹è¯•å’Œéƒ¨ç½²çš„æ—¶é—´ã€‚\n",
      "\n",
      "12. **å¿ƒç†è°ƒé€‚**ï¼š\n",
      "    - å­¦ä¼šæ”¾æ¾å¿ƒæƒ…ï¼Œé€‚æ—¶è°ƒæ•´å¿ƒæ€ã€‚\n",
      "    - å½“é‡åˆ°åŽ‹åŠ›æ—¶ï¼Œå¯»æ±‚åŒäº‹æˆ–ä¸“ä¸šäººå£«çš„å¸®åŠ©ã€‚\n",
      "\n",
      "é€šè¿‡ä»¥ä¸Šæ–¹æ³•ï¼Œæ‚¨å¯ä»¥é€æ­¥æé«˜å·¥ä½œæ•ˆçŽ‡ï¼Œä»Žè€Œæ›´å¥½åœ°å®Œæˆå·¥ä½œä»»åŠ¡ã€‚è®°ä½ï¼Œæ¯ä¸ªäººçš„æƒ…å†µä¸åŒï¼Œæ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ–¹æ³•æ˜¯å…³é”®ã€‚\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "    \n",
    "    print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "    t.join()\n",
    "    return generatetxt\n",
    "\n",
    "user_input = \"ä½œä¸ºä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œå¦‚ä½•æé«˜å·¥ä½œæ•ˆçŽ‡\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0529d814-6e22-4f74-a21a-fdf06a538e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:tensor([[151331, 151333, 151336,    198, 109377,   3837, 117392, 101494, 151337,\n",
      "            198, 109377,   6313, 101494, 103448, 106486, 101119, 111069, 122231,\n",
      "           3837, 100469, 104547, 108895,   3837,  98316, 103799, 107832, 100265,\n",
      "          99228,  99849,   1773, 120249,  99546, 101494, 102769, 114272,  48139,\n",
      "             16,     13,   3070,  99316, 100880,    334,   5122, 101494,  98318,\n",
      "         100166, 104836,     15, 126930,   3837, 100685, 102071,  99103,  98595,\n",
      "         100131,   5373,  98630, 100050,   5373, 111574,  98327, 100392,  98485,\n",
      "         102402,  98401,  98641,   1773,  99281,   3837,  99628, 100039, 102209,\n",
      "         107408, 124171, 116495,   3407,     17,     13,   3070, 125109,    334,\n",
      "          28213,    256,    481,  99743,  98611, 101676,   5122, 120069, 111506,\n",
      "         101676, 102254,   3837,  99002, 116743, 101245, 104667,  99849,   8994,\n",
      "            256,    481, 107020, 102384, 101676,   5122,  98478, 125833, 118940,\n",
      "          98314, 101676, 102254,   3837, 100200, 126686,   3837, 110482,  99995,\n",
      "         100269,   8994,    256,    481, 111302,  98324, 119271, 111549,   5122,\n",
      "         102066,  98729,     18,     22,  98334,  99799, 100540,  98762, 101331,\n",
      "         100113, 104986, 101494,  98324, 119271, 100527,   8994,    256,    481,\n",
      "          10231,    112,    104, 109285, 113344,  98670,   5122,  98549, 104451,\n",
      "         104363,  98400, 106798,  99480,  99849,   3407,     18,     13,   3070,\n",
      "          98917, 100133,    334,   5122, 101494, 104814, 113560, 112223,   3837,\n",
      "         108198, 105136, 118600,   5373,  99764,  98357, 100900, 102060,   3837,\n",
      "          98410, 110029,  99802,  98357,  99209, 100379,  98322,   9904, 121025,\n",
      "          26852, 106797, 120747,  24991,  63896, 105572, 127300,  99104,  98763,\n",
      "          98404,   3407,     19,     13,   3070, 103236,    334,   5122,  99068,\n",
      "         114407,  99452, 112205,   3837, 101494, 105532, 101477, 120370,   3837,\n",
      "          99878,  99253, 101927,   3837, 120001,  99820, 122601,  99333,  98997,\n",
      "          99205,  99845, 100164,   3407,     20,     13,   3070,  99772, 101092,\n",
      "            334,   5122, 101494, 125278,   3837, 102557,  98656, 105555, 101740,\n",
      "          98438, 109388, 106067,   3837,  99039,  99252, 101494, 107069,  98618,\n",
      "         114086,  99089, 103982, 113660,  98734,   3407,     21,     13,   3070,\n",
      "         102652,    334,   5122, 101494, 122236, 109652,  99301, 102900,  98916,\n",
      "         101962, 100030,   5373, 114319, 102900,   5373,  98380, 103827,  98742,\n",
      "          98404,   3837, 108432,  98803,   3407, 105580,   3837, 101494, 117368,\n",
      "         123497,   5373, 103067, 109322,   5373,  98897, 106315, 106825,   3837,\n",
      "         100722,  99526,  98491, 101115,  98327, 100290,   1773, 151336]],\n",
      "       device='cuda:0')\n",
      "GLM-4:\n",
      "ä½ å¥½ï¼å—äº¬æ˜¯ä¸­å›½ä¸œéƒ¨çš„ä¸€ä¸ªåŽ†å²æ–‡åŒ–ååŸŽï¼Œä½äºŽæ±Ÿè‹çœä¸­éƒ¨ï¼Œæ˜¯é•¿æ±Ÿä¸‹æ¸¸çš„é‡è¦åŸŽå¸‚ä¹‹ä¸€ã€‚ä»¥ä¸‹æ˜¯å…³äºŽå—äº¬çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯ï¼š\n",
      "\n",
      "1. **åŽ†å²èƒŒæ™¯**ï¼šå—äº¬æœ‰è¶…è¿‡2500å¹´çš„åŽ†å²ï¼Œæ›¾ç»å…ˆåŽæˆä¸ºä¸œå´ã€å—å”ã€æ˜Žæœå’Œä¸­åŽæ°‘å›½çš„éƒ½åŸŽã€‚å› æ­¤ï¼Œè¿™é‡Œæ‹¥æœ‰ä¸°å¯Œçš„æ–‡åŒ–é—äº§å’ŒåŽ†å²é—è¿¹ã€‚\n",
      "\n",
      "2. **æ—…æ¸¸æ™¯ç‚¹**ï¼š\n",
      "   - ä¸­å±±é™µï¼šå­™ä¸­å±±å…ˆç”Ÿçš„é™µå¢“ï¼Œä¹Ÿæ˜¯å—äº¬å¸‚çš„ä¸»è¦æ™¯ç‚¹ä¹‹ä¸€ã€‚\n",
      "   - æ˜Žå­é™µï¼šæ˜Žå¤ªç¥–æœ±å…ƒç’‹çš„é™µå¢“ï¼Œè§„æ¨¡å®å¤§ï¼Œæ°”åŠ¿é›„ä¼Ÿã€‚\n",
      "   - å—äº¬å¤§å± æ€çºªå¿µé¦†ï¼šçºªå¿µ1937å¹´æ—¥æœ¬ä¾µåŽæˆ˜äº‰æœŸé—´å‘ç”Ÿçš„å—äº¬å¤§å± æ€äº‹ä»¶ã€‚\n",
      "   - ç´«é‡‘å±±å¤©æ–‡å°ï¼šä¸­å›½æœ€æ—©çš„å¤©æ–‡è§‚æµ‹æœºæž„ä¹‹ä¸€ã€‚\n",
      "\n",
      "3. **æ–‡åŒ–ç‰¹è‰²**ï¼šå—äº¬çš„æ–‡åŒ–åº•è•´æ·±åŽšï¼Œæœ‰è®¸å¤šè‘—åçš„æ–‡å­¦å®¶ã€æ”¿æ²»å®¶å‡ºç”Ÿåœ¨è¿™é‡Œï¼Œå¦‚æ˜Žä»£å°è¯´å®¶ç½—è´¯ä¸­ï¼ˆè‘—æœ‰ã€Šä¸‰å›½æ¼”ä¹‰ã€‹ï¼‰ã€è¿‘ä»£æ€æƒ³å®¶ä¸¥å¤ç­‰ã€‚\n",
      "\n",
      "4. **ç»æµŽå‘å±•**ï¼šä½œä¸ºé•¿ä¸‰è§’åœ°åŒºçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå—äº¬çš„ç»æµŽå®žåŠ›é›„åŽšï¼Œå·¥ä¸šåŸºç¡€è‰¯å¥½ï¼Œå°¤å…¶åœ¨ç”µå­å’Œä¿¡æ¯äº§ä¸šæ–¹é¢å…·æœ‰æ˜Žæ˜¾ä¼˜åŠ¿ã€‚\n",
      "\n",
      "5. **äº¤é€šçŠ¶å†µ**ï¼šå—äº¬äº¤é€šä¾¿åˆ©ï¼Œæœ‰å¤šæ¡é«˜é€Ÿå…¬è·¯å’Œå›½é“ç©¿è¿‡å¸‚åŒºï¼ŒåŒæ—¶è¿˜æœ‰å—äº¬ç¦„å£å›½é™…æœºåœºæä¾›å›½å†…å¤–èˆªçº¿æœåŠ¡ã€‚\n",
      "\n",
      "6. **ç¾Žé£Ÿ**ï¼šå—äº¬çš„ç‰¹è‰²å°åƒåŒ…æ‹¬é¸­è¡€ç²‰ä¸æ±¤ã€ç›æ°´é¸­ã€å°ç¬¼åŒ…ç­‰ï¼Œå€¼å¾—ä¸€è¯•ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œå—äº¬æ˜¯ä¸€åº§åŽ†å²æ‚ ä¹…ã€é£Žæ™¯ä¼˜ç¾Žã€ç»æµŽç¹è£çš„åŸŽå¸‚ï¼Œå€¼å¾—æ‚¨åŽ»æŽ¢ç´¢å’Œä½“éªŒã€‚None\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    # t.start()\n",
    "\n",
    "    result = model.generate(**generate_kwargs)\n",
    "    print(f\"result:{result}\")\n",
    "    print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "#     t.join()\n",
    "#     return generatetxt\n",
    "\n",
    "user_input = \"ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹å—äº¬\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "# history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71fc67ca-eceb-46a6-a6ab-0ef321b09962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:tensor([[151331, 151333, 151336,    198, 109377, 151337,    198, 109377,   9281,\n",
      "            239,    233,   6313, 118295, 103810,  98406,   3837, 101665, 110368,\n",
      "          99444,  99212,  11314, 151336]], device='cuda:0')\n",
      "\n",
      "ä½ å¥½ðŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n",
      "ä½ å¥½ðŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    result = model.generate(**generate_kwargs)\n",
    "    print(f\"result:{result}\")\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "    return generatetxt\n",
    "\n",
    "user_input = \"ä½ å¥½\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
