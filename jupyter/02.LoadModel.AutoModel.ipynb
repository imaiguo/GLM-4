{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b64626-3618-470d-b8e0-279b2e9e3975",
   "metadata": {},
   "source": [
    "# glm4 stream调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce2761b-1578-4e8d-ad4c-5dcd7a4c3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deeb64de51646679d4cca55bcd25076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer, AutoModel\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "MODEL_PATH = '/opt/Data/ModelWeight/THUDM/glm-4-9b-chat'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    encode_special_tokens=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917c5745-7ef7-4506-896d-fe25b0494fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM-4:\n",
      "孩子每天喜欢玩耍而不太热衷于学习的现象在儿童心理学中是正常且普遍的。以下是对这一现象的一些看法：\n",
      "\n",
      "1. **生理和心理发展特点**：\n",
      "   - 儿童时期是人类大脑发育最快的阶段之一，孩子们在这个时期的注意力、自控力和认知能力还在发展中。\n",
      "   - 玩耍对于孩子的身心健康发展至关重要，它有助于他们探索世界、培养社交技能和解决问题的能力。\n",
      "\n",
      "2. **教育理念**：\n",
      "   - 当前的教育观念越来越强调“快乐学习”，即通过游戏化的方式让孩子在学习中获得乐趣，从而提高他们的学习兴趣和学习效果。\n",
      "   - 强调素质教育而非单纯的应试教育，鼓励孩子在德智体美劳各方面全面发展。\n",
      "\n",
      "3. **家庭教育与引导**：\n",
      "   - 家长可以通过创造良好的家庭环境来激发孩子的学习兴趣，比如提供丰富的图书资源、科学实验器材等。\n",
      "   - 通过亲子活动或参与社会实践等方式，将学习内容融入日常生活，使孩子在实际操作中发现知识的重要性。\n",
      "\n",
      "4. **社会因素**：\n",
      "   - 在竞争激烈的社会环境中，家长可能会对孩子有较高的期望值，这可能导致孩子感到压力重重，不愿意主动去学习。\n",
      "   - 学校的教育方式和评价体系也可能影响学生的学习积极性。\n",
      "\n",
      "5. **平衡玩耍与学习**：\n",
      "   - 作为家长和教育者，应当找到玩耍与学习之间的平衡点，既不剥夺孩子应有的童年乐趣，也不忽视其学业成长。\n",
      "   - 鼓励孩子进行有益身心的户外活动和集体运动，同时合理安排课业负担。\n",
      "\n",
      "总之，对待孩子喜欢玩耍而较少热爱学习的情况，应该采取理解和支持的态度，结合科学的育儿方法，帮助孩子健康成长。在这个过程中，家长的耐心和理解尤为重要。"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "max_length = 8192\n",
    "top_p = 0.8\n",
    "temperature = 0.6\n",
    "stop = StopOnTokens()\n",
    "\n",
    "user_input = \"如何看待小孩子每天都喜欢玩，不太爱学习\"\n",
    "history.append([user_input, \"\"])\n",
    "messages = []\n",
    "\n",
    "for idx, (user_msg, model_msg) in enumerate(history):\n",
    "    if idx == len(history) - 1 and not model_msg:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        break\n",
    "    if user_msg:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    if model_msg:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    timeout=60,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"input_ids\": model_inputs,\n",
    "    \"streamer\": streamer,\n",
    "    \"max_new_tokens\": max_length,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": top_p,\n",
    "    \"temperature\": temperature,\n",
    "    \"stopping_criteria\": StoppingCriteriaList([stop]),\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"eos_token_id\": model.config.eos_token_id,\n",
    "}\n",
    "\n",
    "t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "t.start()\n",
    "\n",
    "print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "for new_token in streamer:\n",
    "    if new_token:\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "        history[-1][1] += new_token\n",
    "\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bf099a-f593-4224-93d9-2396d09b34fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM-4:\n",
      "作为软件工程师，提升工作效率是一个持续的过程。以下是一些建议帮助您提高个人工作效率：\n",
      "\n",
      "1. **时间管理**：\n",
      "   - 使用番茄工作法等技巧来集中注意力。\n",
      "   - 制定清晰的日程表和任务清单。\n",
      "\n",
      "2. **优先级排序**：\n",
      "   - 学会区分紧急与重要的事务，并按照重要性进行排序处理。\n",
      "\n",
      "3. **技术工具**：\n",
      "   - 利用版本控制、项目管理工具（如Jira）、代码审查工具等自动化流程。\n",
      "   - 学习使用快捷键和脚本来自动化重复性任务。\n",
      "\n",
      "4. **编码习惯**：\n",
      "   - 保持良好的编程规范和注释，使代码易于阅读和维护。\n",
      "   - 遵循设计模式和最佳实践。\n",
      "\n",
      "5. **团队协作**：\n",
      "   - 与团队成员保持沟通畅通，及时分享进度和问题。\n",
      "   - 参加代码评审会议，从他人反馈中学习。\n",
      "\n",
      "6. **知识积累**：\n",
      "   - 持续学习和更新自己的技能库。\n",
      "   - 关注行业动态和技术趋势。\n",
      "\n",
      "7. **避免干扰**：\n",
      "   - 在需要专注工作时关闭不必要的通知和社交媒体应用。\n",
      "   - 创建一个有利于工作的环境。\n",
      "\n",
      "8. **健康生活**：\n",
      "   - 保证充足的睡眠和适当的运动。\n",
      "   - 注意饮食均衡，以维持身体和精神状态良好。\n",
      "\n",
      "9. **反思总结**：\n",
      "   - 定期回顾过去的工作表现，找出可以改进的地方。\n",
      "   - 从失败的项目或错误中学到经验教训。\n",
      "\n",
      "10. **合理分工**：\n",
      "    - 了解自己擅长和不擅长的领域，将任务分配给最合适的人选。\n",
      "\n",
      "11. **减少等待时间**：\n",
      "    - 优化开发过程，缩短编译、测试和部署的时间。\n",
      "\n",
      "12. **心理调适**：\n",
      "    - 学会放松心情，适时调整心态。\n",
      "    - 当遇到压力时，寻求同事或专业人士的帮助。\n",
      "\n",
      "通过以上方法，您可以逐步提高工作效率，从而更好地完成工作任务。记住，每个人的情况不同，找到适合自己的方法是关键。\n",
      "作为软件工程师，提升工作效率是一个持续的过程。以下是一些建议帮助您提高个人工作效率：\n",
      "\n",
      "1. **时间管理**：\n",
      "   - 使用番茄工作法等技巧来集中注意力。\n",
      "   - 制定清晰的日程表和任务清单。\n",
      "\n",
      "2. **优先级排序**：\n",
      "   - 学会区分紧急与重要的事务，并按照重要性进行排序处理。\n",
      "\n",
      "3. **技术工具**：\n",
      "   - 利用版本控制、项目管理工具（如Jira）、代码审查工具等自动化流程。\n",
      "   - 学习使用快捷键和脚本来自动化重复性任务。\n",
      "\n",
      "4. **编码习惯**：\n",
      "   - 保持良好的编程规范和注释，使代码易于阅读和维护。\n",
      "   - 遵循设计模式和最佳实践。\n",
      "\n",
      "5. **团队协作**：\n",
      "   - 与团队成员保持沟通畅通，及时分享进度和问题。\n",
      "   - 参加代码评审会议，从他人反馈中学习。\n",
      "\n",
      "6. **知识积累**：\n",
      "   - 持续学习和更新自己的技能库。\n",
      "   - 关注行业动态和技术趋势。\n",
      "\n",
      "7. **避免干扰**：\n",
      "   - 在需要专注工作时关闭不必要的通知和社交媒体应用。\n",
      "   - 创建一个有利于工作的环境。\n",
      "\n",
      "8. **健康生活**：\n",
      "   - 保证充足的睡眠和适当的运动。\n",
      "   - 注意饮食均衡，以维持身体和精神状态良好。\n",
      "\n",
      "9. **反思总结**：\n",
      "   - 定期回顾过去的工作表现，找出可以改进的地方。\n",
      "   - 从失败的项目或错误中学到经验教训。\n",
      "\n",
      "10. **合理分工**：\n",
      "    - 了解自己擅长和不擅长的领域，将任务分配给最合适的人选。\n",
      "\n",
      "11. **减少等待时间**：\n",
      "    - 优化开发过程，缩短编译、测试和部署的时间。\n",
      "\n",
      "12. **心理调适**：\n",
      "    - 学会放松心情，适时调整心态。\n",
      "    - 当遇到压力时，寻求同事或专业人士的帮助。\n",
      "\n",
      "通过以上方法，您可以逐步提高工作效率，从而更好地完成工作任务。记住，每个人的情况不同，找到适合自己的方法是关键。\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "    \n",
    "    print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "    t.join()\n",
    "    return generatetxt\n",
    "\n",
    "user_input = \"作为一名软件工程师，如何提高工作效率\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0529d814-6e22-4f74-a21a-fdf06a538e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:tensor([[151331, 151333, 151336,    198, 109377,   3837, 117392, 101494, 151337,\n",
      "            198, 109377,   6313, 101494, 103448, 106486, 101119, 111069, 122231,\n",
      "           3837, 100469, 104547, 108895,   3837,  98316, 103799, 107832, 100265,\n",
      "          99228,  99849,   1773, 120249,  99546, 101494, 102769, 114272,  48139,\n",
      "             16,     13,   3070,  99316, 100880,    334,   5122, 101494,  98318,\n",
      "         100166, 104836,     15, 126930,   3837, 100685, 102071,  99103,  98595,\n",
      "         100131,   5373,  98630, 100050,   5373, 111574,  98327, 100392,  98485,\n",
      "         102402,  98401,  98641,   1773,  99281,   3837,  99628, 100039, 102209,\n",
      "         107408, 124171, 116495,   3407,     17,     13,   3070, 125109,    334,\n",
      "          28213,    256,    481,  99743,  98611, 101676,   5122, 120069, 111506,\n",
      "         101676, 102254,   3837,  99002, 116743, 101245, 104667,  99849,   8994,\n",
      "            256,    481, 107020, 102384, 101676,   5122,  98478, 125833, 118940,\n",
      "          98314, 101676, 102254,   3837, 100200, 126686,   3837, 110482,  99995,\n",
      "         100269,   8994,    256,    481, 111302,  98324, 119271, 111549,   5122,\n",
      "         102066,  98729,     18,     22,  98334,  99799, 100540,  98762, 101331,\n",
      "         100113, 104986, 101494,  98324, 119271, 100527,   8994,    256,    481,\n",
      "          10231,    112,    104, 109285, 113344,  98670,   5122,  98549, 104451,\n",
      "         104363,  98400, 106798,  99480,  99849,   3407,     18,     13,   3070,\n",
      "          98917, 100133,    334,   5122, 101494, 104814, 113560, 112223,   3837,\n",
      "         108198, 105136, 118600,   5373,  99764,  98357, 100900, 102060,   3837,\n",
      "          98410, 110029,  99802,  98357,  99209, 100379,  98322,   9904, 121025,\n",
      "          26852, 106797, 120747,  24991,  63896, 105572, 127300,  99104,  98763,\n",
      "          98404,   3407,     19,     13,   3070, 103236,    334,   5122,  99068,\n",
      "         114407,  99452, 112205,   3837, 101494, 105532, 101477, 120370,   3837,\n",
      "          99878,  99253, 101927,   3837, 120001,  99820, 122601,  99333,  98997,\n",
      "          99205,  99845, 100164,   3407,     20,     13,   3070,  99772, 101092,\n",
      "            334,   5122, 101494, 125278,   3837, 102557,  98656, 105555, 101740,\n",
      "          98438, 109388, 106067,   3837,  99039,  99252, 101494, 107069,  98618,\n",
      "         114086,  99089, 103982, 113660,  98734,   3407,     21,     13,   3070,\n",
      "         102652,    334,   5122, 101494, 122236, 109652,  99301, 102900,  98916,\n",
      "         101962, 100030,   5373, 114319, 102900,   5373,  98380, 103827,  98742,\n",
      "          98404,   3837, 108432,  98803,   3407, 105580,   3837, 101494, 117368,\n",
      "         123497,   5373, 103067, 109322,   5373,  98897, 106315, 106825,   3837,\n",
      "         100722,  99526,  98491, 101115,  98327, 100290,   1773, 151336]],\n",
      "       device='cuda:0')\n",
      "GLM-4:\n",
      "你好！南京是中国东部的一个历史文化名城，位于江苏省中部，是长江下游的重要城市之一。以下是关于南京的一些基本信息：\n",
      "\n",
      "1. **历史背景**：南京有超过2500年的历史，曾经先后成为东吴、南唐、明朝和中华民国的都城。因此，这里拥有丰富的文化遗产和历史遗迹。\n",
      "\n",
      "2. **旅游景点**：\n",
      "   - 中山陵：孙中山先生的陵墓，也是南京市的主要景点之一。\n",
      "   - 明孝陵：明太祖朱元璋的陵墓，规模宏大，气势雄伟。\n",
      "   - 南京大屠杀纪念馆：纪念1937年日本侵华战争期间发生的南京大屠杀事件。\n",
      "   - 紫金山天文台：中国最早的天文观测机构之一。\n",
      "\n",
      "3. **文化特色**：南京的文化底蕴深厚，有许多著名的文学家、政治家出生在这里，如明代小说家罗贯中（著有《三国演义》）、近代思想家严复等。\n",
      "\n",
      "4. **经济发展**：作为长三角地区的重要组成部分，南京的经济实力雄厚，工业基础良好，尤其在电子和信息产业方面具有明显优势。\n",
      "\n",
      "5. **交通状况**：南京交通便利，有多条高速公路和国道穿过市区，同时还有南京禄口国际机场提供国内外航线服务。\n",
      "\n",
      "6. **美食**：南京的特色小吃包括鸭血粉丝汤、盐水鸭、小笼包等，值得一试。\n",
      "\n",
      "总之，南京是一座历史悠久、风景优美、经济繁荣的城市，值得您去探索和体验。None\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    # t.start()\n",
    "\n",
    "    result = model.generate(**generate_kwargs)\n",
    "    print(f\"result:{result}\")\n",
    "    print(\"GLM-4:\", end=\"\", flush=True)\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "#     t.join()\n",
    "#     return generatetxt\n",
    "\n",
    "user_input = \"你好，介绍一下南京\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "# history[-1][1] = history[-1][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71fc67ca-eceb-46a6-a6ab-0ef321b09962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:tensor([[151331, 151333, 151336,    198, 109377, 151337,    198, 109377,   9281,\n",
      "            239,    233,   6313, 118295, 103810,  98406,   3837, 101665, 110368,\n",
      "          99444,  99212,  11314, 151336]], device='cuda:0')\n",
      "\n",
      "你好👋！很高兴见到你，有什么可以帮助你的吗？\n",
      "你好👋！很高兴见到你，有什么可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = model.config.eos_token_id\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "history = []\n",
    "\n",
    "def runChat(user_input:str):\n",
    "    history.append([user_input, \"\"])\n",
    "    messages = []\n",
    "    \n",
    "    for idx, (user_msg, model_msg) in enumerate(history):\n",
    "        if idx == len(history) - 1 and not model_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            break\n",
    "        if user_msg:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if model_msg:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "    \n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        timeout=60,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"input_ids\": model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\":  0.6,\n",
    "        \"stopping_criteria\": StoppingCriteriaList([StopOnTokens()]),\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"eos_token_id\": model.config.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    result = model.generate(**generate_kwargs)\n",
    "    print(f\"result:{result}\")\n",
    "\n",
    "    generatetxt=\"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token:\n",
    "            print(new_token, end=\"\", flush=True)\n",
    "            history[-1][1] += new_token\n",
    "            generatetxt+=new_token\n",
    "    return generatetxt\n",
    "\n",
    "user_input = \"你好\"\n",
    "res = runChat(user_input)\n",
    "print(res)\n",
    "history[-1][1] = history[-1][1].strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
